---
layout: distill
title: Lecture Notes Template
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-01-09

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Donghan Yu  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Songwei Ge
    url: "#"
  - name: Xiaofei Shi
    url: "#"
  - name: Aniketh Janardhan Reddy
    url: "http://anikethjr.github.io "
    
editors:
  - name: Maruan Al-Shedivat
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Section 1 (0-20 mins, by Aniketh)

## Section 2 (20-40 mins, by Songwei)

## Section 3 (40-60 mins, by Donghan)

## Section 4 (60-80 mins, by Fei)

## Recap of the message passing algorithm and its properties

In the previous lectures, we have looked at exact inference algorithms and observed that they are not very efficient. Hence, a need for more efficient inference algorithms arises. This lecture will focus on such algorithms which are called Approximate Inference algorithms.

Inference using graphical models can be used to compute marginal distributions, conditional distributions, the likelihood of observed data, and the modes of the density function. We have already studied that exact inference can be accomplished either using brute force (i.e. eliminating all the required variables in any order) or by refining our elimination order so as to reduce the number of computations. In this context, the belief propagation or sum-product message passing algorithm run on a clique tree generated by a given variable elimination ordering was introduced as an equivalent way of performing variable elimination. We have seen that the overall complexity of the algorithm is exponential in the number of variables in the largest elimination clique which is generated when we use a given elimination order. The tree width of a graph was defined as one less than the smallest possible value of the cardinality of the largest elimination clique, ranging over all possible elimination orderings. If we can find an optimal elimination order, we can reduce the complexity of belief propagation. As the problem of finding the best elimination order is NP-hard, exact inference is also NP-hard. Belief propagation is guaranteed to converge to a unique and fixed set of values after a finite number of iterations when it is run on trees.

### Message Passing Protocol

The message passing protocol dictates that a node can only send a message to its neighbours when and only when it has received the messages from all of its other neighbours. Hence, to naively compute the marginal of a given node, we should treat that node as the root and run the message passing algorithm. This is illustrated by the three figures given below, each of which shows the messaging directions to be used when computing the given marginal:

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm4.png' | relative_url }}" />
        </div>
    </div>
</figure>

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm5.png' | relative_url }}" />
        </div>
    </div>
</figure>

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm6.png' | relative_url }}" />
        </div>
    </div>
</figure>

### Message Passing for HMMs

When the message passing algorithm is applied to a HMM shown below, we will see that the forward and backward algorithms can be obtained.

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm7.png' | relative_url }}" />
        </div>
    </div>
</figure>

The corresponding clique tree for the HMM shown above is:

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm8.png' | relative_url }}" />
        </div>
    </div>
</figure>

Now, the messages (denoted by $$\mu$$s) and the potentials (denoted by $$\psi$$s) involved in the rightward pass are depicted by the below figure:

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm9.png' | relative_url }}" />
        </div>
    </div>
</figure>

We have that,

$$
\mu_{t\rightarrow t+1} (y_{t+1}) = \sum_{y_t} \psi(y_t,y_{t+1})\mu_{t-1\rightarrow t}(y_t)\mu_{t\uparrow}(y_{t+1})
$$

We know that:  

$$\psi(y_t,y_{t+1}) = p(y_{t+1}|y_t) = a_{y_t,y_{t+1}}$$ 

is the probability of transitioning from $$y_t$$ to $$y_{t+1}$$ and 

$$\mu_{t\uparrow}(y_{t+1}) = p(x_{t+1}|y_{t+1})$$ 

is the probability of emitting $$x_{t+1}$$ in state $$y_{t+1}$$.

$$
\Longrightarrow \mu_{t\rightarrow t+1} (y_{t+1}) = \sum_{y_t} p(y_{t+1}|y_t)\mu_{t-1\rightarrow t}(y_t)p(x_{t+1}|y_{t+1}) = p(x_{t+1}|y_{t+1}) \sum_{y_t} a_{y_t,y_{t+1}} \mu_{t-1\rightarrow t}(y_t)
$$

which is the forward algorithm.

Similarly, the messages and the potentials involved in the leftward pass are depicted by the below figure:

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm10.png' | relative_url }}" />
        </div>
    </div>
</figure>

Then we have that,
$$
\mu_{t-1\leftarrow t}(y_t) = \sum_{y_{t+1}} \psi(y_t,y_{t+1}) \mu_{t\leftarrow t+1}(y_{t+1})\mu_{t\uparrow}(y_{t+1}) = \sum_{y_{t+1}} p(y_{t+1}|y_t) \mu_{t\leftarrow t+1}(y_{t+1}) p(x_{t+1}|y_{t+1})
$$

which is the backward algorithm.


### Correctness of Belief Propagation in Trees

**Theorem:** The message passing algorithm correctly computes all of the marginals in a tree.  

This is a result of there being only one unique path between any two nodes in a tree. Intuitively, this guarantees that only two unique messages can be associated with an edge, one for each direction of traversal.

### Local and Global Consistency

Let $$\{\tau_C, C \in \mathcal{C}\}$$ and $$\{\tau_S, S \in \mathcal{S}\}$$ denote the set of functions which are associated with cliques and separator sets respectively.

These sets of functions are locally consistent if the following properties hold:
- $$\sum_{x'_S} \tau_S(x'_S) = 1, \forall S \in \mathcal{S}$$  
- $$\sum_{x'_C|x'_S = x_S} \tau_C(x'_C) = \tau_S(x_S), \forall C \in \mathcal{C}, \forall S \subset C$$  

The first property implies that the functions associated with each separator set are proper marginals. The second property requires that if we sum any clique function $$\tau_C$$ over all the variables in the clique $$C$$ which are not present in a sepset $$S \subset C$$, we must obtain a function $$\tau_S(X_S)$$.

The aforementioned sets of functions are global consistent if all $$\tau_C$$ and $$\tau_S$$ are valid marginals.

For junction trees, local consistency is equivalent to global consistency. A proof of this fact can be found at this [link](http://www.cs.princeton.edu/courses/archive/spring09/cos513/scribe/lecture07.pdf).

However, for graphs which are not trees, local consistency does not always imply global consistency.

**Example**

Consider the two following message passing sequences for the same graph:

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm1.png' | relative_url }}" />
        </div>
    </div>
</figure>

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm2.png' | relative_url }}" />
        </div>
    </div>
</figure>

It can be seen that we obtain different values for $$P(A)$$ based on the message passing sequence.

Similarly, if we construct a clique tree for the above graph (shown below), we see that the random variable $$C$$ is part of two non-neighbouring cliques. Hence, it is impossible for the two clique potentials which contain $$C$$ to agree on the marginal associated with $$C$$ since no information about $$C$$ is ever passed in the messages.

<figure id="basic-structure" class="l-body">
    <div class="row">
        <div class="col two">
            <img src="{{ '/assets/img/notes/lecture-12/pgm3.png' | relative_url }}" />
        </div>
    </div>
</figure>

<!-- Consider the case where the $$\tau_C$$s and $$\tau_S$$s are marginal probability distributions. In this case, the second property is satisfied because $$\sum_{X_1,X_2 | X_2 = x_2} P(X_1,X_2) = P(X_2 = x_2)$$. -->




## Theory of Variational Inferece

### Inference Problems in Graphical Models

Given an undirected graphical model, i.e.

$$
p(x) = \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi_C(x_C),
$$

where $\mathcal{C}$ denotes the collection of cliques, one is interested in the inference of the marginal distributions

$$
p(x_i) = \sum_{x_j, j\neq i} p(x).
$$

### Ingredients: Exponential Families

Definition: We say $X$ follows from an exponential family provided that the parametrized collection of density functions satisfies:

$$
p(x;\theta) = \exp\left\{ \theta^T\phi(x) - A(\theta) \right\}, \qquad A(\theta)<\infty .
$$

Moreover, $phi$ is one of the sufficient statistics for $\theta$, see Larry Wasserman's lecture notes from 10/36-705 for more details and examples; and $A$ is usually known as the log partition function, which is convex and lower semi-continuous. Further, 

<d-math block>
\begin{aligned}
A(\theta) &= \log \mathbb{E}_{\theta} \left[ \exp\{\theta^T\phi(X)\}\tight], \\
\frac{\partial A(\theta)}{\partial \theta} &= \mathbb{E}_{\theta} \left[ \phi(X) \right]. 
\end{aligned}
</d-math>

* Parametrize Graphical Models as Exponential Family
For undirected graphical model, if we assume

$$
\log \psi_C(x_C) = \log \psi(x_C;\theta_C) = \theta_C^T \phi(x_C),
$$

and let

$$
A(\theta) = \log Z(\theta)
$$

then

$$
p(x;\theta) = \exp( \sum_{C \in \mathcal{C}} \theta_C^T \phi(x_C)- A(\theta)).
$$

* Exmaples: Gaussian MRF, Discrete MRF. 


### Ingredients: Convex Conjugate

Definition: For a function $f$, the convex conjugate dual, which is also known as the Legendre transform of $f$, is defined as

$$
f^* (\mu) = \sup_{\theta} \{ \theta^T\mu - f(\theta)\},
$$

and the convex conjugate dual is convex, no matter the original function is convex or not, and
Moreover, if $f$ is convex and lower semi-continuous, then 

$$
f(x) = \sup_{\mu} \{ \theta^T\mu - f^* (\mu)\}.
$$

* Application to Exponential Family
Let $A$ be the log partition function for the exponential family

$$
p(x;\theta) = \exp\left\{ \theta^T\phi(x) - A(\theta) \right\}, 
$$

The dual for $A$ is 

$$
A^* (\mu) = \sup\{ \theta^T \mu - A(\theta): A(\theta)<\infty\},
$$

and the stationarity condition is,

<d-math block>
\begin{aligned}
\mu = \frac{\partial A(\theta)}{\partial \theta} = \mathbb{E}_{\theta} \left[ \phi(X) \right],
\end{aligned}
<d-math block>

we can thus represent $\theta$ through the mean parameter $\mu$.
Therefore, we have the following Legendre mapping:

<d-math block>
\begin{aligned}
A^* (\mu) = \mathbb{E}_{\theta(\mu)} \left[ \log p(X;\theta(\mu)) \right] = -H(p(X;\theta(\mu))),
\end{aligned}
<d-math block>

where $H$ is the Boltzmann-Shannon entropy function. 

### Ingredients: Convex Polytope

Half-plane Representation: the Minkowski-Weyl Theorem

Theorem: A non-empty convex polytope $\mathcal{M}$ can be characterized by a finite collection of linear inequality constraints, i.e.  

$$
\mathcal{M} = \left\{ \mu: a_j^T \mu \geq b_j, j \in \mathcal{J} \right\}, \qquad |\mathcal{J}| <\infty. 
$$


### Marginal Polytope

Definition: For a distribution $p(x)$ and a sufficient statistics $\phi(x)$, the mean parameter is defined as:

<d-math block>
\begin{aligned}
\mu = \mathbb{E}_p[\phi(X)],
\end{aligned}
<d-math block>

and the set of all realizable mean parameters is denoted by:

<d-math block>
\begin{aligned}
\mathcal{M} := \left\{ \mu: \mathbb{E}_p[\phi(X)] = \mu, \text{for some distribution $p$} \right\}.
\end{aligned}
<d-math block>

* $\mathcal{M}$ is a convex set; 

* For discrete exponential families, $\mathcal{M}$ is called the marginal polytope, and has the following convex hull representation:

<d-math block>
\begin{aligned}
\mathcal{M} = conv\{ \phi(x): x\in \mathcal{X}^m} \}.
\end{aligned}
<d-math block>
  By the Minkowski-Weyl Theorem, the marginal polytope can be represented by a finite collection of linear inequality constraints, see the examples for the 2-node Ising model.
