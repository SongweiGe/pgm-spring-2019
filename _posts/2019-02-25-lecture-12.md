---
layout: distill
title: Lecture Notes Template
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-01-09

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Donghan Yu  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Songwei Ge
    url: "#"
  - name: Xiaofei Shi
    url: "#"
  - name: Aniketh Janardhan Reddy
    url: "#"
    
editors:
  - name: Maruan Al-Shedivat
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Section 1 (0-20 mins, by Aniketh)

## Section 2 (20-40 mins, by Songwei)

## Section 3 (40-60 mins, by Donghan)

## Section 4 (60-80 mins, by Fei)

## Theory of Variational Inferece

### Inference Problems in Graphical Models

Given an undirected graphical model, i.e.

$$
p(x) = \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi_C(x_C),
$$

where $\mathcal{C}$ denotes the collection of cliques, one is interested in the inference of the marginal distributions

$$
p(x_i) = \sum_{x_j, j\neq i} p(x).
$$

### Ingredients: Exponential Families

Definition: We say $X$ follows from an exponential family provided that the parametrized collection of density functions satisfies:

$$
p(x;\theta) = \exp\left\{ \theta^T\phi(x) - A(\theta) \right\}, \qquad A(\theta)<\infty .
$$

Moreover, $phi$ is one of the sufficient statistics for $\theta$, see Larry Wasserman's lecture notes from 10/36-705 for more details and examples; and $A$ is usually known as the log partition function, which is convex and lower semi-continuous. Further, 

<d-math block>
\begin{aligned}
A(\theta) &= \log \mathbb{E}_{\theta} \left[ \exp\{\theta^T\phi(X)\}\tight], \\
\frac{\partial A(\theta)}{\partial \theta} &= \mathbb{E}_{\theta} \left[ \phi(X) \right]. 
\end{aligned}
</d-math>

* Parametrize Graphical Models as Exponential Family
For undirected graphical model, if we assume

$$
\log \psi_C(x_C) = \log \psi(x_C;\theta_C) = \theta_C^T \phi(x_C),
$$

and let

$$
A(\theta) = \log Z(\theta)
$$

then

$$
p(x;\theta) = \exp( \sum_{C \in \mathcal{C}} \theta_C^T \phi(x_C)- A(\theta)).
$$

* Exmaples: Gaussian MRF, Discrete MRF. 


### Ingredients: Convex Conjugate

Definition: For a function $f$, the convex conjugate dual, which is also known as the Legendre transform of $f$, is defined as

$$
f^* (\mu) = \sup_{\theta} \{ \theta^T\mu - f(\theta)\},
$$

and the convex conjugate dual is convex, no matter the original function is convex or not, and
Moreover, if $f$ is convex and lower semi-continuous, then 

$$
f(x) = \sup_{\mu} \{ \theta^T\mu - f^* (\mu)\}.
$$

* Application to Exponential Family
Let $A$ be the log partition function for the exponential family

$$
p(x;\theta) = \exp\left\{ \theta^T\phi(x) - A(\theta) \right\}, 
$$

The dual for $A$ is 

$$
A^* (\mu) = \sup\{ \theta^T \mu - A(\theta): A(\theta)<\infty\},
$$

and the stationarity condition is,

<d-math block>
\begin{aligned}
\mu = \frac{\partial A(\theta)}{\partial \theta} = \mathbb{E}_{\theta} \left[ \phi(X) \right],
\end{aligned}
<d-math block>

we can thus represent $\theta$ through the mean parameter $\mu$.
Therefore, we have the following Legendre mapping:

<d-math block>
\begin{aligned}
A^* (\mu) = \mathbb{E}_{\theta(\mu)} \left[ \log p(X;\theta(\mu)) \right] = -H(p(X;\theta(\mu))),
\end{aligned}
<d-math block>

where $H$ is the Boltzmann-Shannon entropy function. 

### Ingredients: Convex Polytope

Half-plane Representation: the Minkowski-Weyl Theorem

Theorem: A non-empty convex polytope $\mathcal{M}$ can be characterized by a finite collection of linear inequality constraints, i.e.  

$$
\mathcal{M} = \left\{ \mu: a_j^T \mu \geq b_j, j \in \mathcal{J} \right\}, \qquad |\mathcal{J}| <\infty. 
$$


### Marginal Polytope

Definition: For a distribution $p(x)$ and a sufficient statistics $\phi(x)$, the mean parameter is defined as:

<d-math block>
\begin{aligned}
\mu = \mathbb{E}_p[\phi(X)],
\end{aligned}
<d-math block>

and the set of all realizable mean parameters is denoted by:

<d-math block>
\begin{aligned}
\mathcal{M} := \left\{ \mu: \mathbb{E}_p[\phi(X)] = \mu, \text{for some distribution $p$} \right\}.
\end{aligned}
<d-math block>

* $\mathcal{M}$ is a convex set; 

* For discrete exponential families, $\mathcal{M}$ is called the marginal polytope, and has the following convex hull representation:

<d-math block>
\begin{aligned}
\mathcal{M} = conv\{ \phi(x): x\in \mathcal{X}^m} \}.
\end{aligned}
<d-math block>
  By the Minkowski-Weyl Theorem, the marginal polytope can be represented by a finite collection of linear inequality constraints, see the examples for the 2-node Ising model.
